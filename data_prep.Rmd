---
title: "MVA Final Project"
author:
- Javier Ferrando Monsonis
- Marcel Porta Valles
- Mehmet Fatih ??agil
date: "February 20, 2018"
output: 
  pdf_document: 
    keep_tex: yes
---

```{r}

library(magrittr)
library(ggplot2)
library(dplyr)
library(stringr)
library(ggplot2)

library(data.table)
library(dplyr)
library(magrittr)
library(ggplot2)
library(gridExtra)
library(ggExtra)
library(corrplot)
library(factoextra)
library(stringr)
library(FactoMineR)
#library(kableExtra)
library(knitr)

```

```{r, include = FALSE}

setwd("/Users/JaviFerrando/Desktop/ML-Project/")
dir <- '/Users/JaviFerrando/Desktop/ML-Project/input/'

sample_submission <- read.csv(paste(dir,'SampleSubmissionStage1.csv',sep=''))#2014-2018 every possible matchup 


# Get data
dseeds_tournament <- fread(paste(dir,'NCAATourneySeeds.csv',sep=''))
dg_tournment <- fread(paste(dir,'NCAATourneyCompactResults.csv',sep=''))


# keep only season, daynum, win and loss team ids for the dg_tournament data
outcome_tournament <- dg_tournment %>% select(Season, DayNum, WTeamID, LTeamID)
names(outcome_tournament) <- tolower(names(outcome_tournament))

# randomize winning and losing team into team 1 and team 2 (necessary for probabilities later) and drop other ids
outcome_tournament <- outcome_tournament %>% 
  mutate(rand = runif(dim(outcome_tournament)[1]), 
         team1id = ifelse(rand >= 0.5, wteamid, lteamid),
         team2id = ifelse(rand <0.5, wteamid, lteamid),
         team1win = ifelse(team1id == wteamid, 1, 0)) %>% 
  select(-rand, -wteamid,-lteamid)

# Add seeding information to games: 

# make seeds 1-16 without letters (except for certain seed)
dseeds_tournament <- dseeds_tournament %>% 
  mutate(ranking = as.factor((str_replace(Seed, "[A-Z]",""))), 
         rank_num = as.numeric(str_replace(ranking, "[a-z]","")))
names(dseeds_tournament) <- tolower(names(dseeds_tournament))

# team 1
outcome_tournament <- outcome_tournament %>% 
  left_join(
    select(dseeds_tournament, t1_rank = ranking, t1_rank_n = rank_num, teamid, season), 
    by = c("team1id"="teamid","season"="season")) 

# team 2
outcome_tournament <- outcome_tournament %>% 
  left_join(
    select(dseeds_tournament, t2_rank = ranking, t2_rank_n = rank_num, teamid, season), 
    by = c("team2id"="teamid","season"="season")) 


# replace NA seeds
outcome_tournament <- outcome_tournament %>% mutate(t1_rank = ifelse(is.na(t1_rank), 8.5, t1_rank),
                                                    t2_rank = ifelse(is.na(t2_rank), 8.5, t2_rank),
                                                    t1_rank_n = ifelse(is.na(t1_rank_n), 8.5, t1_rank_n),
                                                    t2_rank_n = ifelse(is.na(t2_rank_n), 8.5, t2_rank_n),
                                                    diff_rank = t1_rank_n - t2_rank_n)



season_elos <- read.csv(paste(dir,'season_elos.csv',sep='')) %>% rename(teamid = team_id)


#Add season_elos (for t1 and t2) to outcome tournament
# Join team 1 data
outcome_tournament <- outcome_tournament %>% 
  left_join(
    select(season_elos, 
           season, 
           teamid, 
           t1_season_elo = season_elo),
    by = c("team1id" = "teamid","season" = "season"))


# Join team 2 data
outcome_tournament <- outcome_tournament %>% 
  left_join(
    select(season_elos, 
           season, 
           teamid, 
           t2_season_elo = season_elo),
    by = c("team2id" = "teamid","season" = "season"))


# Compute ELO probabilities for the game, and the difference in ELO scores

outcome_tournament <- outcome_tournament %>% 
  mutate(elo_diff = t1_season_elo - t2_season_elo,
         elo_prob_1 = 1/(10^(-elo_diff/400)+1)
  )

################################
outcome_tournament <- outcome_tournament[outcome_tournament$season>=2003,]


################################################################################################################################

#Add advanced statistics

seas_enrich <- fread(paste(dir,'NCAASeasonDetailedResultsEnriched.csv',sep=''))

win_stats <- seas_enrich[, .(
  Season,
  TeamID = WTeamID,
  Result = rep('W', .N),
  FGM = WFGM,
  FGA = WFGA,
  FGP = WFGM / WFGA,
  FGP2 = (WFGM - WFGM3) / (WFGA - WFGA3),
  FGM3 = WFGM3,
  FGA3 = WFGA3,
  FGP3 = WFGM3 / WFGA3,
  FTM = WFTM,
  FTA = WFTA,
  FTP = WFTM / WFTA,
  OR = WOR,
  DR = WDR,
  AST = WAst,
  TO = WTO,
  STL = WStl,
  BLK = WBlk,
  PF = WPF,
  PIE = WPIE,
  ORP = WOR / (WOR + LDR),
  DRP = WDR / (WDR + LOR),
  eFG = WeFGP,
  NetRTG = WNetRtg,
  POS = 0.96 * (WFGA + WTO + 0.44 * WFTA - WOR)
)]

los_stats <- seas_enrich[, .(
  Season,
  TeamID = LTeamID,
  Result = rep('L', .N),
  FGM = LFGM,
  FGA = LFGA,
  FGP = LFGM / LFGA,
  FGP2 = (LFGM - LFGM3) / (LFGA - LFGA3),
  FGM3 = LFGM3,
  FGA3 = LFGA3,
  FGP3 = LFGM3 / LFGA3,
  FTM = LFTM,
  FTA = LFTA,
  FTP = LFTM / LFTA,
  OR = LOR,
  DR = LDR,
  AST = LAst,
  TO = LTO,
  STL = LStl,
  BLK = LBlk,
  PF = LPF,
  PIE = LPIE,
  ORP = (LOR / (LOR + WDR)),
  DRP = LDR / (LDR + WOR),
  eFG = LeFGP,
  NetRTG = LNetRtg,
  POS = 0.96 * (LFGA + LTO + 0.44 * LFTA - LOR)
)]

stats_all <- rbindlist(list(win_stats, los_stats))


stats_season <- stats_all[, .(
  FGP = sum(FGM) / sum(FGA),
  FGP3 = sum(FGM3) / sum(FGA3),
  FTP = sum(FTM) / sum(FTA),
  ORPG = mean(OR),
  DRPG = mean(DR),
  ASPG = mean(AST),
  TOPG = mean(TO),
  STPG = mean(STL),
  #BLPG = mean(BLK),
  #PFPG = mean(PF),
  MeFG = mean(eFG),
  MNetRTG = mean(NetRTG),
  #MORP = mean(ORP),
  MPIE = mean(PIE),
  MPOS = mean(POS),
  EFG = (mean(FGM)+0.5*mean(FGM3))/mean(FGA))
  
  , by = c('TeamID', 'Season')]

################################################################################################################################
#MPIE feature

# Join team 1 data
outcome_tournament <- outcome_tournament %>% 
  left_join(
    select(stats_season, 
           Season, 
           TeamID, 
           t1_mpie = MPIE),
    by = c("team1id" = "TeamID","season" = "Season"))

# Join team 2 data
outcome_tournament <- outcome_tournament %>% 
  left_join(
    select(stats_season, 
           Season, 
           TeamID, 
           t2_mpie = MPIE),
    by = c("team2id" = "TeamID","season" = "Season"))

################################
#Netrtg feature

# Join team 1 data
outcome_tournament <- outcome_tournament %>% 
  left_join(
    select(stats_season, 
           Season, 
           TeamID, 
           t1_netrtg = MNetRTG),
    by = c("team1id" = "TeamID","season" = "Season"))

# Join team 2 data
outcome_tournament <- outcome_tournament %>% 
  left_join(
    select(stats_season, 
           Season, 
           TeamID, 
           t2_netrtg = MNetRTG),
    by = c("team2id" = "TeamID","season" = "Season"))



### Load data

#sample_submission <- read.csv(paste(dir,'SampleSubmissionStage2.csv',sep=''))#2019 every possible matchup -> can only check by submitting to Kaggle


#####################################################################################################
#d_ss -> same as outcome_tournament but with sample_submission format (every possible matchup)

### Join team data and ranking data

d_ss <- sample_submission


# Add season, team1id and team2id columns from sample submission ID
d_ss <- d_ss %>% mutate(season = as.numeric(gsub("(.*)_(.*)_(.*)",ID, replacement = "\\1")), 
                        team1id =  as.numeric(gsub("(.*)_(.*)_(.*)",ID, replacement = "\\2")),
                        team2id =  as.numeric(gsub("(.*)_(.*)_(.*)",ID, replacement = "\\3")))

# Add rank data

# team 1
d_ss <- d_ss %>% 
  left_join(
    dplyr::select(dseeds_tournament, t1_rank = ranking, t1_rank_n = rank_num, teamid, season), 
    by = c("team1id"="teamid","season"="season")) 

# team 2
d_ss <- d_ss %>% 
  left_join(
    dplyr::select(dseeds_tournament, t2_rank = ranking, t2_rank_n = rank_num, teamid, season), 
    by = c("team2id"="teamid","season"="season")) 



### Join ELO rating data
#season_elos <- read.csv("../input/fivethirtyeight-elo-ratings/season_elos.csv") %>% rename(teamid = team_id)
season_elos <- read.csv(paste(dir,'season_elos.csv',sep='')) %>% rename(teamid = team_id)

# Join team 1 data
d_ss <- d_ss %>% 
  left_join(
    select(season_elos, 
           season, 
           teamid, 
           t1_season_elo = season_elo),
    by = c("team1id" = "teamid","season" = "season"))


# Join team 2 data
d_ss <- d_ss %>% 
  left_join(
    select(season_elos, 
           season, 
           teamid, 
           t2_season_elo = season_elo),
    by = c("team2id" = "teamid","season" = "season"))


# Key differences between winner and loser
#Add elop probability

d_ss <- d_ss %>% 
  mutate(elo_diff = t1_season_elo - t2_season_elo,
         elo_prob_1 = 1/(10^(-elo_diff/400)+1),
         diff_rank = t1_rank_n - t2_rank_n
  )

#PIE feature
d_ss <- d_ss[d_ss$season>=2003,]
# Join team 1 data
d_ss <- d_ss %>% 
  left_join(
    select(stats_season, 
           Season, 
           TeamID, 
           t1_mpie = MPIE),
    by = c("team1id" = "TeamID","season" = "Season"))

# Join team 2 data
d_ss <- d_ss %>% 
  left_join(
    select(stats_season, 
           Season, 
           TeamID, 
           t2_mpie = MPIE),
    by = c("team2id" = "TeamID","season" = "Season"))

################################
#Netrtg

# Join team 1 data
d_ss <- d_ss %>% 
  left_join(
    select(stats_season, 
           Season, 
           TeamID, 
           t1_netrtg = MNetRTG),
    by = c("team1id" = "TeamID","season" = "Season"))

# Join team 2 data
d_ss <- d_ss %>% 
  left_join(
    select(stats_season, 
           Season, 
           TeamID, 
           t2_netrtg = MNetRTG),
    by = c("team2id" = "TeamID","season" = "Season"))


d_ss$t1_rank <- NULL
d_ss$t2_rank <- NULL
outcome_tournament$t1_rank <- NULL
outcome_tournament$t2_rank <- NULL

# d_ss$t1_rank_n <- NULL
# d_ss$t2_rank_n <- NULL
# d_ss$elo_diff <- NULL
### Make predictions based on model 
train <- outcome_tournament %>% filter(season <= 2013) #Takes occurred tournament games results (team1win)
# train$t1_rank_n <- NULL
# train$t2_rank_n <- NULL
# train$elo_diff <- NULL



test_outcome_tournament <- outcome_tournament %>% filter(season > 2013) #Test sample, target team1win
```

```{r}
#train$daynum <- NULL
#train$t1_rank <- NULL
#train$t2_rank <- NULL
kable(train[sample(nrow(train), 6), ][,1:10])

```

```{r}
training_set <- train[,-(1:4)]
col_order <- colnames(training_set)

test_set<- d_ss[,-1][,-(2:4)]
colnames(test_set)[1] <- "team1win"
test_set <- test_set[, col_order]
```

```{r}

#Train model with train data
#Add predictions to dss
#Merge d_ss with test_outcome_tournament (games that occurred) -> validation
#validation has target and Pred for every game that occurered 2014-2018
#Apply LogLoss to validation$Pred and validation$team1win


#logistic regression model: differences
model <- glm(team1win ~ 
               diff_rank +
               t1_rank_n +
               #t2_rank_n +
               t1_season_elo +
               t2_season_elo +
               #elo_prob_1 +
               t1_mpie +
               t2_mpie +
               t1_netrtg +
               t2_netrtg
             ,
             
             data = training_set, family = binomial)

model <- glm(team1win ~ .
             ,
             
             data = training_set, family = binomial)


#Predict on every possible matchup
predict <- data.frame(Pred = predict(model, newdata = test_set, type = 'response'))
d_ss <- d_ss %>% mutate(Pred = predict$Pred)# %>% dplyr::select(ID, Pred) Change sample submission pred=0.5 to model predicition

d_ss_fin <- sample_submission %>% mutate(Pred = d_ss$Pred) #only matchup and prediction -> Results for kaggle
#write.csv(d_ss_fin, "submission_stage_2.csv", row.names = FALSE)

summary(model)
```

```{r}
#Regularized Logistic Regression
#Total fail-> predictions wrong

set.seed(123)
library(glmnet)
x <- model.matrix(team1win~., training_set)
y <- training_set$team1win
cv.lasso <- cv.glmnet(x, y , alpha = 1, family = "binomial")
# Fit the final model on the training data
model <- glmnet(x, y, alpha = 1, family = "binomial",
                lambda = cv.lasso$lambda.min)
plot(cv.lasso)
# Display regression coefficients
coef(model)

#### glmnet test

x.test <- model.matrix(team1win~., test_set)

probabilities <- model %>% predict(newx = x.test)
```

```{r}
#Neural Network
#Scale inputs
library(nnet)
library(caret)
library(neuralnet)


train_nnet_scaled <- as.data.frame(scale(training_set[-1]))
train_nnet_scaled <- cbind(training_set$team1win,train_nnet_scaled)
colnames(train_nnet_scaled)[1] <- "team1win"
#train_nnet_scaled$team1win<- factor(train_nnet_scaled$team1win, labels=c(0,1))#not for neuralnet

#nn1 <- nnet(team1win~t1_rank_n+t2_rank_n+elo_diff,data=train_nnet_scaled,entropy=T,size=100,decay=0,maxit=1000,trace=T)
#predict <- data.frame(Pred = predict(nn1, newdata = d_ss_nnet_scaled))

#10 fold cv trial

## We first split the available data into learning and test sets, selecting randomly 2/3 and 1/3 of the data
## We do this for a honest estimation of prediction performance

names <- colnames(train_nnet_scaled)[-1] #choose the names you want
a <- as.formula(paste('team1win ~ ' ,paste(names,collapse='+')))
a
#neuralnet DOESN'T need factors as target
nn <- neuralnet(a, data=train_nnet_scaled, hidden=c(1), linear.output=FALSE, threshold=0.01)
nn$result.matrix
plot(nn)



#nnet by means of train function, needs factors as target
# model <- train(team1win~., data=train_nnet_scaled, method='nnet', maxit = 300,
#                trControl=trainControl(method='cv'))
test_set_scaled <- scale(test_set)

#predict <- data.frame(Pred = predict (model, newdata=d_ss_nnet_scaled, type="prob"))

test_set_scaled <- subset(test_set_scaled, select = colnames(test_set_scaled)[-1])
nn.results <- compute(nn, test_set_scaled)

#train in nnet prediction
d_ss <- d_ss %>% mutate(Pred = predict$Pred.1)# %>% dplyr::select(ID, Pred) Change sample submission pred=0.5 to model predicition

#neuralnet prediction
#d_ss$Pred <- NULL
d_ss <- d_ss %>% mutate(Pred = as.numeric(nn.results$net.result))





d_ss_fin <- sample_submission %>% mutate(Pred = d_ss$Pred) #only matchup and prediction -> Results for kaggle
#write.csv(d_ss_fin, "submission_stage_2.csv", row.names = FALSE)


#############
set.seed(43)
N <- nrow(train_nnet_scaled)
learn <- sample(1:N, round(2*N/3))
(sizes <- 2*seq(1,10,by=2)) #different sizes

## specify 10x10 CV
trc <- trainControl (method="repeatedcv", number=10, repeats=10)

model.10x10CV <- train (team1win ~., data = train_nnet_scaled, subset=learn, method='nnet', maxit = 500, trace = FALSE,
                        tuneGrid = expand.grid(.size=sizes,.decay=0), trControl=trc)

(decays <- 10^seq(-3,0,by=0.1))

```

```{r}
#We can only test with 2014-2018 data
#Merge every possible matchup result predictions with real games and check test error
test_result <- merge(x = test_outcome_tournament, y = d_ss[2:5], by=c("team1id","team2id","season"), all = FALSE)

library(MLmetrics)
#library(forecast)

LogLoss(y_pred = test_result$Pred, y_true = test_result$team1win)

test_result$Pred<- factor(test_result$Pred, labels=c(0,1))#
Accuracy(y_pred = test_result$Pred, y_true = test_result$team1win)

```

```{r,echo=FALSE}
library(rpart)
train_tree <- training_set
train_tree$team1win<- factor(train_tree$team1win, labels=c(0,1))#not for neuralnet

DecisionTree = rpart(team1win ~ ., data=train_tree,control=rpart.control(cp=0.001, xval=10),method='class')
printcp(DecisionTree)

treeSize = DecisionTree$cptable[,2]+1 #nsplit
treeImpurity = DecisionTree$cptable[,3] #rel error
cvImpurity = DecisionTree$cptable[,4] #xerror

plot(treeSize, treeImpurity, main="R(T)", xlab="size of the tree", ylab="Relativity Impurity", type="o", col='red') 
lines(treeSize, cvImpurity ,type="o", col='blue')
legend("topright", c("All training data","CV training data"), col=c('red', 'blue'), lty=1)
```

```{r, echo=FALSE}
DecisionTree$cptable = as.data.frame(DecisionTree $cptable)
ind = which.min(DecisionTree$cptable$xerror)
xerr <-DecisionTree$cptable$xerror[ind]
xstd <-DecisionTree$cptable$xstd[ind]

i = 1
while (DecisionTree$cptable$xerror[i] > xerr+xstd){
  i = i+1
}
#alfa = DecisionTree$cptable$CP[i]
alfa = DecisionTree$cptable$CP[3]

optimal <- prune(DecisionTree, cp=alfa)
par(mfrow = c(1,1), xpd = NA)
plot(optimal)
text(optimal, use.n=T,cex=0.8,col="blue")

#Tree prediction
rpart_pred <- predict(DecisionTree,test_set,type='prob')[,1]
rpart_pred_class <- predict(DecisionTree,test_set,type='class')
d_ss <- d_ss %>% mutate(Pred = predict(DecisionTree,test_set,type='prob')[,1])
#d_ss <- d_ss %>% mutate(Pred = predict(DecisionTree,test_set,type='class'))
```

```{r, echo=FALSE}
library(randomForest)
train_tree <- training_set
train_tree$team1win<- factor(train_tree$team1win, labels=c(0,1))#not for neuralnet

#Convert d_ss_tree$team1win to categorical values
test_set_rf <- test_set
test_set_rf$team1win <- NULL
test_set_rf$team1win <- sample(c(0, 1), nrow(test_set_rf), replace=TRUE)

test_set_rf <- test_set_rf[, col_order]
test_set_rf$team1win<- factor(test_set_rf$team1win, labels=c(0,1))

random_forest <- randomForest(formula = team1win ~.,
                        data=train_tree,
                        mtry=3,      # three predictor-vars selected randomly at each split
                        xtest=test_set_rf[-1],
                        ytest=test_set_rf$team1win,
                        #ytest=as.factor(audit_imp$Adjusted[testRows]),
                        importance=T,
                        ntree=500,   # acceptably large value to ensure each sample row is predicted
                                     # at least 2-digit nbr of times on average
                        nodesize = 50,
                        maxnodes = 40,
                        norm.votes=T,
                        keep.forest=TRUE)

#rf_predictions_prob <- predict(random_forest, test_set_rf, type='prob')
rf_predictions_class <- predict(random_forest, test_set_rf, type='class')
#d_ss <- d_ss %>% mutate(Pred = rf_predictions_prob[,2])#For prob
d_ss <- d_ss %>% mutate(Pred = rf_predictions_class)
# 
# cf <- confusionMatrix(factor(df_rf_predictions), factor(rf_test$target), positive="1", dnn = c("Prediction", "True values"))
# draw_confusion_matrix(cf)
```


```{r, echo=FALSE}
library(caret)
train_tree <- train[,-(1:4)]
#train_tree$team1win<- factor(train_tree$team1win, labels=c(0,1))
train_tree$team1win<- factor(train_tree$team1win, labels=c("win","loss"))#not for neuralnet

# Example of Bagging algorithms
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "logLoss"
# Bagged CART
set.seed(seed)
fit.treebag <- train(team1win~., data=train_tree, method="treebag", metric=metric, trControl=control)
# Random Forest
set.seed(seed)
fit.rf <- train(team1win~., data=train_tree, method="rf", metric=metric, trControl=control)
# summarize results
bagging_results <- resamples(list(treebag=fit.treebag, rf=fit.rf))
summary(bagging_results)
dotplot(bagging_results)
```

```{r, echo=FALSE}

# Example of Stacking algorithms
# create submodels
train_ensemble <- train_tree
train_ensemble$diff_rank <- NULL
train_ensemble$elo_diff <- NULL
train_ensemble$team1win<- factor(train_ensemble$team1win, labels=c("win","loss"))#not for neuralnet

library(caretEnsemble)
control <- trainControl(method="repeatedcv", number=10, repeats=10, savePredictions='all', classProbs=TRUE,summaryFunction = mnLogLoss)
algorithmList <- c('lda', 'glm', 'svmRadial')#knn disaster
#algorithmList <- c('rpart', 'glm','svmRadial')
set.seed(7)
metric <- "logLoss"
models <- caretList(team1win~., data=train_ensemble, trControl=control, methodList=algorithmList, metric=metric)



greedy_ensemble <- caretEnsemble(
  models, 
  metric="logLoss",
  trControl=control)
summary(greedy_ensemble)

kable(modelCor(resamples(models)))

summary(greedy_ensemble)
results <- resamples(models)
summary(results)
dotplot(results)
ensemble_pred <- predict(greedy_ensemble, newdata=test_set,type='prob')
d_ss <- d_ss %>% mutate(Pred = ensemble_pred)#F
```